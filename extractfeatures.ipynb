{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c81033-9210-42e0-8e65-c850857f6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Mokymasis visą gyvenimą tapo politikos prioritetu , o valstybių narių švietimo ir mokymo sistemų reformomis siekiama didinti investicijas į žmogiškąjį kapitalą , sudaryti palankesnes sąlygas inovacijoms ir skatinti verslumo kultūrą .\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input, output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4c1599d-bbcb-4996-bdab-26c450f181af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [ 0.3180,  0.6312, -0.5047,  ...,  0.2151,  0.0246, -0.1945],\n",
      "         [-0.1757,  0.1410,  0.2358,  ...,  0.6386,  0.4809, -0.1323],\n",
      "         ...,\n",
      "         [-0.3308,  0.8271, -0.2800,  ..., -0.4470, -0.5362,  0.9812],\n",
      "         [-0.4498,  0.3225,  0.5143,  ...,  0.1424,  0.3526,  0.3998],\n",
      "         [-0.5649, -0.1463,  0.5240,  ..., -0.7546, -0.1358, -0.2930]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1061, -0.0247, -0.2049,  ...,  0.2179,  0.0355, -0.0201],\n",
      "         [ 0.2456,  1.2804, -0.8012,  ...,  0.5985, -0.0414, -0.2635],\n",
      "         [-0.7609, -0.4656,  0.1074,  ...,  0.6016,  0.6009,  0.1062],\n",
      "         ...,\n",
      "         [ 0.2051,  1.1009, -0.8263,  ..., -0.7857, -0.7319,  0.8676],\n",
      "         [-0.4237, -0.2488,  0.2439,  ...,  0.0347, -0.0556,  0.0653],\n",
      "         [-0.2385, -0.5702,  0.1014,  ..., -0.1559,  0.0966, -0.2295]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0083, -0.2070, -0.3018,  ...,  0.3959,  0.1007, -0.0386],\n",
      "         [-0.1137,  1.7064, -1.1570,  ...,  0.4604,  0.3570, -0.4750],\n",
      "         [-1.3149, -0.5163, -0.3485,  ..., -0.0909,  0.5725,  0.0788],\n",
      "         ...,\n",
      "         [ 0.1242,  1.4984, -0.7302,  ..., -0.8757, -0.9023,  0.8245],\n",
      "         [-0.6297, -0.2238,  0.3666,  ..., -0.1551, -0.1248,  0.0452],\n",
      "         [-0.2304, -0.5028,  0.0853,  ..., -0.1084,  0.1764, -0.5427]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0787, -0.2051, -0.1803,  ...,  0.4209,  0.2077,  0.2092],\n",
      "         [ 0.2596,  1.5726, -0.3180,  ...,  0.1503,  0.5146, -0.5111],\n",
      "         [-1.3838, -0.7165, -0.0705,  ..., -0.7294,  0.8034,  0.6646],\n",
      "         ...,\n",
      "         [ 0.7673,  1.1644, -0.0167,  ..., -0.4428, -1.0644,  0.9048],\n",
      "         [-0.5613, -0.3982,  0.6298,  ..., -0.0655, -0.2860, -0.1451],\n",
      "         [-0.0825, -0.1335,  0.0768,  ...,  0.0553,  0.0414, -0.0809]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3630, -0.6797, -0.5168,  ...,  0.3974,  0.2682,  0.4584],\n",
      "         [ 0.7493,  0.8815, -0.2260,  ...,  0.4370, -0.1419, -0.6086],\n",
      "         [-1.1891, -0.9515, -0.1267,  ..., -0.5135, -0.0658,  0.5172],\n",
      "         ...,\n",
      "         [ 1.0540,  1.0996, -0.4187,  ...,  0.0836, -0.9945,  0.4655],\n",
      "         [-0.5052, -0.4366, -0.1441,  ...,  0.2017, -0.1434, -0.0785],\n",
      "         [-0.0034, -0.0816,  0.0090,  ...,  0.0277,  0.0482, -0.0251]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1793, -0.8728, -0.4152,  ..., -0.2525,  0.1731,  0.6521],\n",
      "         [ 0.4547,  0.7773,  0.1646,  ..., -0.7970, -0.0228, -0.4678],\n",
      "         [-1.0015, -0.5587,  0.3834,  ..., -0.7745, -0.3304,  0.8874],\n",
      "         ...,\n",
      "         [ 1.5826,  0.9783, -0.3494,  ..., -0.1425, -1.3173,  0.7238],\n",
      "         [-0.5025, -0.7701, -0.5386,  ..., -0.6074, -0.6856, -0.2506],\n",
      "         [-0.0096, -0.0551,  0.0315,  ..., -0.0077,  0.0231, -0.0360]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.6494e-01, -1.3896e+00, -2.5793e-01,  ..., -2.1516e-01,\n",
      "           2.0013e-01,  6.4044e-01],\n",
      "         [ 2.4061e-01,  4.0869e-01,  3.3617e-01,  ..., -1.1094e+00,\n",
      "          -7.3160e-02, -8.0863e-01],\n",
      "         [-8.7686e-01, -6.7113e-01,  5.6757e-01,  ..., -9.0412e-01,\n",
      "          -4.6439e-01,  6.0790e-01],\n",
      "         ...,\n",
      "         [ 1.3377e+00,  1.0120e+00, -6.6908e-01,  ..., -2.7529e-01,\n",
      "          -1.3267e+00,  5.0129e-01],\n",
      "         [-5.2312e-01, -7.5277e-01, -5.7269e-01,  ..., -6.0611e-01,\n",
      "          -1.1342e+00,  1.4741e-01],\n",
      "         [ 2.4543e-02, -5.5280e-02, -2.6413e-04,  ..., -2.9593e-02,\n",
      "          -3.6153e-03, -2.3737e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2185, -1.0256,  0.0199,  ..., -0.2040,  0.1446,  0.8737],\n",
      "         [ 0.4247,  1.1212,  0.4902,  ..., -0.8517,  0.4527, -0.3103],\n",
      "         [-0.7725, -0.5402,  0.6187,  ..., -0.4846, -0.5376,  0.2185],\n",
      "         ...,\n",
      "         [ 1.5779,  1.4347, -0.1484,  ..., -0.5035, -0.7091,  0.9406],\n",
      "         [-0.3465, -0.7253, -1.2708,  ..., -0.4212, -0.8261,  1.1737],\n",
      "         [ 0.0433,  0.0296, -0.0080,  ..., -0.0408,  0.0255, -0.0123]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0829, -0.7750, -0.3548,  ..., -0.4037,  0.2370,  1.1333],\n",
      "         [ 0.2943,  0.9342, -0.0180,  ..., -0.3508,  0.6316,  0.0585],\n",
      "         [-0.6287, -0.3253,  0.3539,  ..., -0.2471, -0.9825, -0.1464],\n",
      "         ...,\n",
      "         [ 0.8115,  0.8334, -0.2188,  ..., -0.4033, -0.3868,  0.7272],\n",
      "         [-1.0120, -0.6454, -1.3832,  ..., -0.6581, -0.8685,  0.8355],\n",
      "         [ 0.0440,  0.0353,  0.0513,  ..., -0.0305, -0.0227, -0.0189]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1475, -0.6538, -0.2708,  ..., -0.2086,  0.1331,  0.8615],\n",
      "         [ 0.2761,  0.9830,  0.0983,  ..., -0.4253,  0.1869, -0.3141],\n",
      "         [-0.5554, -0.4611,  0.4226,  ..., -0.6954, -1.2046, -0.5960],\n",
      "         ...,\n",
      "         [ 0.7950,  0.5749, -0.3150,  ..., -0.7721, -0.7290,  0.3716],\n",
      "         [-1.0212, -0.5118, -1.3530,  ..., -0.4960, -0.9015,  0.3137],\n",
      "         [ 0.0822,  0.1070,  0.0137,  ..., -0.1073, -0.0711, -0.0266]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2241, -0.7215, -0.1213,  ..., -0.3700, -0.0523,  0.8350],\n",
      "         [ 0.2336,  0.9056,  0.4894,  ..., -0.1437,  0.1254, -0.7611],\n",
      "         [-0.6352, -0.2340,  0.1468,  ..., -1.0206, -1.3072, -0.7865],\n",
      "         ...,\n",
      "         [ 0.8299,  0.5024, -0.7976,  ..., -0.8217, -1.0362,  0.4145],\n",
      "         [-0.6314, -0.4129, -0.9865,  ..., -0.4414, -0.6729,  0.1882],\n",
      "         [ 0.0284,  0.0147, -0.1080,  ...,  0.0123, -0.0520,  0.0109]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.4390,  0.2056,  0.1078,  ..., -0.0601, -0.4137,  0.2127],\n",
      "         [ 0.2613,  0.6779,  0.3465,  ...,  0.3732,  0.2218, -0.8910],\n",
      "         [-0.4889, -0.1094,  0.2936,  ..., -0.6500, -1.3133, -0.8566],\n",
      "         ...,\n",
      "         [ 0.9503,  1.2277, -0.8767,  ..., -0.8286, -1.4790,  0.3576],\n",
      "         [-0.5852, -0.4680, -0.8424,  ...,  0.1744, -0.7912, -0.5299],\n",
      "         [ 0.0480,  0.0407, -0.0435,  ...,  0.0087, -0.0036,  0.0032]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.4508,  0.4317, -0.2599,  ..., -0.0923,  0.0065,  0.4387],\n",
      "         [-0.0298,  0.3362,  0.1621,  ...,  0.3183,  0.3232, -0.2379],\n",
      "         [-0.4238, -0.1052,  0.0614,  ..., -0.2048, -0.6311, -0.4867],\n",
      "         ...,\n",
      "         [ 0.5592,  0.5068, -0.8455,  ..., -0.4246, -0.4625,  0.2050],\n",
      "         [-0.4714, -0.3110, -0.6551,  ...,  0.4282, -0.0764, -0.1171],\n",
      "         [ 0.2962, -0.0105, -0.3779,  ...,  0.3372, -0.2869, -0.1839]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>))\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(output))\n",
    "hidden_states = output[2]\n",
    "print(hidden_states)\n",
    "print(len(hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a75eca89-1b83-47c6-8355-d3e253c78f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [ 0.3180,  0.6312, -0.5047,  ...,  0.2151,  0.0246, -0.1945],\n",
      "         [-0.1757,  0.1410,  0.2358,  ...,  0.6386,  0.4809, -0.1323],\n",
      "         ...,\n",
      "         [-0.3308,  0.8271, -0.2800,  ..., -0.4470, -0.5362,  0.9812],\n",
      "         [-0.4498,  0.3225,  0.5143,  ...,  0.1424,  0.3526,  0.3998],\n",
      "         [-0.5649, -0.1463,  0.5240,  ..., -0.7546, -0.1358, -0.2930]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "embedding_output = hidden_states[0]\n",
    "print(embedding_output)\n",
    "print(len(embedding_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2416ed58-0e7f-46c4-b94c-c35f793f5f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(input_file):\n",
    "  \"\"\"Read a list of `InputExample`s from an input file.\"\"\"\n",
    "  examples = []\n",
    "  unique_id = 0\n",
    "  with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "    while True:\n",
    "      line = reader.readline()\n",
    "      if not line:\n",
    "          break\n",
    "      text = line.strip().split('\\t')[-1]\n",
    "      examples.append(text)\n",
    "      unique_id += 1\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df98675-a0a0-4f34-a285-8ae06eb12d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\adoma\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m(2266)\u001b[0;36membedding\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m   2264 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   2265 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m-> 2266 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   2267 \u001b[1;33mdef embedding_bag(\n",
      "\u001b[0m\u001b[1;32m   2268 \u001b[1;33m    \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\adoma\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m(2266)\u001b[0;36membedding\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m   2264 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   2265 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m-> 2266 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   2267 \u001b[1;33mdef embedding_bag(\n",
      "\u001b[0m\u001b[1;32m   2268 \u001b[1;33m    \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\adoma\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m(2266)\u001b[0;36membedding\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m   2264 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   2265 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m-> 2266 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m   2267 \u001b[1;33mdef embedding_bag(\n",
      "\u001b[0m\u001b[1;32m   2268 \u001b[1;33m    \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  continue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0780, -0.0454, -0.0454,  ..., -0.0535, -0.0218, -0.0535],\n",
      "         [ 0.1749,  0.6618, -0.1256,  ..., -0.0904,  0.1683, -0.2197],\n",
      "         [-0.9231,  0.0877,  0.3768,  ..., -0.3216, -0.2217, -0.0118],\n",
      "         ...,\n",
      "         [-0.0898,  0.3302, -0.0684,  ..., -0.1578, -0.1485, -0.1357],\n",
      "         [-0.4545,  0.8917, -0.2299,  ...,  0.7790, -0.1886, -1.1163],\n",
      "         [-0.4443,  0.9073, -0.2187,  ...,  0.8316, -0.1154, -1.1627]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-6.3372e-02,  1.4227e-01,  5.2355e-02,  8.2032e-02,  1.1336e-01,\n",
      "          2.4476e-01, -9.0115e-04,  8.7838e-02, -8.4573e-02,  8.2633e-02,\n",
      "         -5.7566e-02, -1.6131e-02,  7.5042e-02, -8.4975e-02,  7.0461e-03,\n",
      "          8.1162e-03, -2.7818e-02, -6.4721e-02, -2.6112e-03,  2.3068e-02,\n",
      "         -3.0404e-02,  2.5914e-02, -7.6890e-02,  1.5580e-01,  1.0996e-01,\n",
      "         -3.9749e-02,  1.1072e-01, -6.2109e-03, -1.6938e-01,  1.8329e-01,\n",
      "          7.6925e-02,  1.0106e-01,  1.6869e-01, -2.1238e-02,  1.5415e-01,\n",
      "         -5.2804e-02,  9.7104e-02,  7.6606e-02,  2.0459e-01, -3.1516e-02,\n",
      "          6.8813e-03,  4.7143e-03,  7.5220e-02, -2.3989e-02, -1.9911e-01,\n",
      "         -7.5310e-02, -8.5399e-02, -3.2111e-03,  9.9996e-01,  8.7660e-02,\n",
      "         -3.4325e-02,  1.8605e-01, -6.7466e-03, -1.9423e-01,  1.2621e-01,\n",
      "          9.9997e-01, -2.2405e-01, -1.8686e-01,  2.7315e-02, -3.8155e-02,\n",
      "         -7.4270e-02, -3.2998e-02,  1.5895e-01,  8.2653e-02,  4.2217e-02,\n",
      "          1.0867e-01, -1.2849e-03,  1.9168e-01,  1.4073e-01,  5.4569e-03,\n",
      "          5.4467e-02,  1.5598e-02,  1.8774e-01,  1.5347e-01, -5.5610e-02,\n",
      "         -5.3951e-02, -1.3020e-02, -5.3819e-02, -8.1938e-02,  1.3063e-01,\n",
      "         -3.4772e-02,  1.1356e-02, -1.1659e-01,  1.2865e-01,  8.7838e-02,\n",
      "         -6.1426e-02, -3.4966e-02,  5.3623e-02, -7.2562e-03, -1.3077e-01,\n",
      "         -3.3766e-02, -8.1317e-02, -2.4634e-02, -6.5809e-02,  7.0331e-02,\n",
      "         -7.5800e-02, -5.2716e-02,  2.5479e-02, -5.8925e-02, -7.3659e-02,\n",
      "          1.7239e-02,  1.1707e-01, -2.1551e-02,  1.6718e-01,  1.2032e-01,\n",
      "         -1.7785e-01, -2.4938e-03, -8.6193e-02,  5.3982e-02, -9.3428e-02,\n",
      "         -1.3085e-01, -3.7769e-02, -5.0526e-02,  5.3104e-02,  8.7186e-03,\n",
      "         -2.5882e-02, -2.5478e-02, -1.4362e-01,  5.8497e-03, -2.2010e-02,\n",
      "         -1.9788e-01,  9.9997e-01, -5.4854e-02, -3.3845e-02,  8.7501e-02,\n",
      "         -1.0285e-01,  3.2145e-02,  1.3293e-01, -1.2355e-01,  5.9141e-02,\n",
      "         -2.3362e-01,  1.4067e-02, -1.3823e-01, -4.4376e-02, -1.1466e-01,\n",
      "          1.6746e-01,  2.9608e-02, -1.5289e-02, -6.0059e-02, -1.4418e-01,\n",
      "          1.3866e-02,  2.8499e-02,  4.1310e-02, -3.8968e-02,  5.9456e-02,\n",
      "          8.4081e-02, -4.1311e-02, -1.0718e-01,  1.2216e-01,  6.0681e-02,\n",
      "         -2.0624e-01, -6.2875e-02, -1.5791e-01, -5.5706e-02,  8.0814e-02,\n",
      "         -1.5242e-01, -4.1670e-02, -1.5692e-01,  9.1309e-02, -9.9521e-02,\n",
      "         -1.2976e-01,  9.4007e-02,  9.5614e-02, -3.6275e-02,  8.0576e-03,\n",
      "          1.3139e-02, -2.9033e-02, -4.8656e-02,  9.6195e-02, -1.4492e-01,\n",
      "          1.1576e-01,  4.2887e-02,  4.9424e-02,  5.2176e-02, -5.5341e-02,\n",
      "         -1.1932e-01,  6.1720e-02,  4.8506e-02,  2.9362e-02,  5.8583e-02,\n",
      "         -5.4258e-02, -1.8246e-01, -4.3816e-02,  4.8684e-02,  8.1302e-02,\n",
      "          9.8463e-03, -6.8882e-03,  1.0138e-01, -1.5953e-01, -1.0799e-01,\n",
      "          8.5924e-02,  2.8604e-02,  1.7332e-03,  1.0814e-01, -1.3669e-02,\n",
      "          1.8681e-01, -5.9504e-02, -5.8680e-02,  2.0640e-02,  2.1805e-02,\n",
      "         -9.8561e-02, -1.3469e-01,  1.4728e-01,  5.0333e-02, -1.8182e-01,\n",
      "          1.5337e-01, -2.4974e-02,  9.9997e-01,  1.8301e-01,  1.1832e-01,\n",
      "          4.6926e-02,  5.5104e-02,  1.0044e-01, -7.0244e-03,  3.1175e-02,\n",
      "          1.0980e-01, -1.0723e-03, -5.5130e-02,  9.8147e-02, -5.8159e-02,\n",
      "         -1.1415e-01,  3.0353e-03,  1.0256e-01, -1.3710e-01,  1.2524e-01,\n",
      "         -1.1989e-01, -1.0478e-01, -1.9688e-01, -1.0084e-01,  1.5709e-01,\n",
      "         -4.5825e-02,  4.6874e-02,  2.7003e-01, -2.1008e-01,  6.0342e-02,\n",
      "         -2.0317e-01, -1.2358e-01, -1.3524e-01,  9.7782e-02, -3.3171e-02,\n",
      "          6.4514e-02,  1.1322e-01, -5.0863e-02, -6.0570e-02,  1.5096e-01,\n",
      "         -7.1365e-02,  1.6274e-02, -4.8446e-02, -5.3578e-02, -8.1797e-02,\n",
      "          4.0066e-02,  5.7763e-02, -1.5080e-02,  2.1434e-01,  6.2852e-02,\n",
      "         -2.7502e-03, -3.1193e-02, -5.4035e-02,  1.3832e-02,  6.8138e-02,\n",
      "         -3.2618e-03, -1.2268e-01, -9.6146e-02, -3.8331e-02,  1.0518e-01,\n",
      "         -1.8336e-01, -1.2428e-01, -7.7094e-02,  1.6981e-02, -2.5677e-02,\n",
      "         -1.6807e-01,  1.0545e-01,  3.6284e-02,  1.1489e-01, -2.6317e-02,\n",
      "          6.4657e-02, -1.5349e-02, -2.1584e-01,  1.9008e-01, -6.2554e-02,\n",
      "          1.8617e-01,  1.1019e-02, -1.2139e-01, -2.2917e-02,  7.4563e-03,\n",
      "          1.1818e-01,  3.7720e-02, -1.7241e-02,  9.8435e-02,  9.5184e-02,\n",
      "          1.4836e-02,  3.2058e-02, -3.2964e-03, -5.3078e-02,  6.8612e-02,\n",
      "         -1.1632e-01,  1.9366e-01,  5.3998e-02, -1.4545e-02,  8.4129e-03,\n",
      "          1.3766e-02, -1.2115e-01, -9.7766e-03,  8.0907e-02,  8.2196e-02,\n",
      "          1.2063e-01, -1.5035e-01,  5.3124e-02, -6.3831e-03,  3.1831e-02,\n",
      "          1.7866e-02,  1.3062e-02, -1.8229e-01,  1.5342e-01, -2.2238e-01,\n",
      "         -1.4934e-02,  5.5357e-02, -1.1457e-01,  3.4073e-02,  1.2440e-01,\n",
      "         -3.9472e-02,  9.9997e-01, -8.0223e-02, -5.2947e-02, -9.9997e-01,\n",
      "         -2.8136e-01, -1.6249e-01,  8.9410e-02,  1.2368e-02, -1.4707e-01,\n",
      "          3.0885e-03,  4.2615e-02, -6.0150e-02, -3.3529e-02, -1.9796e-01,\n",
      "          1.1945e-01, -8.7280e-02, -2.9008e-02,  1.2188e-02, -2.2193e-02,\n",
      "          1.9790e-02, -9.9995e-01,  1.4212e-01,  6.5745e-02,  1.1225e-01,\n",
      "          6.4093e-03, -1.2975e-01, -1.1027e-01, -2.0553e-02,  4.9031e-02,\n",
      "          4.1026e-02, -1.7793e-02,  1.3774e-01, -5.9296e-02, -2.1836e-01,\n",
      "          8.9514e-02,  4.3921e-02, -3.4970e-02, -6.2088e-02, -7.0004e-02,\n",
      "         -9.1424e-02,  1.1335e-01, -1.1645e-01,  2.2294e-01, -1.4485e-02,\n",
      "         -5.7275e-02, -7.7470e-02,  9.9997e-01,  7.4379e-02, -1.3389e-02,\n",
      "          9.9997e-01,  1.5888e-01,  3.5120e-02, -4.1219e-02,  8.5941e-02,\n",
      "         -1.2773e-02,  9.9997e-01, -1.6674e-02, -1.5450e-01,  5.8145e-02,\n",
      "         -1.6732e-01,  1.0281e-01,  1.0914e-02, -1.4625e-01, -9.9996e-01,\n",
      "          4.2080e-02,  1.9359e-02,  1.7094e-01,  6.6411e-02,  9.9997e-01,\n",
      "          4.9957e-02,  1.0433e-01, -7.8469e-03,  2.5976e-01,  4.8815e-03,\n",
      "          2.1727e-01,  1.3507e-01,  9.0980e-02, -1.2579e-01,  2.6903e-02,\n",
      "          7.5766e-02, -1.7653e-01, -1.8542e-01,  3.6930e-02, -1.3007e-02,\n",
      "          9.2358e-02, -1.6860e-01, -3.2095e-02,  2.5678e-02,  9.9996e-01,\n",
      "         -1.1364e-01,  6.8379e-02, -2.0569e-02, -5.7143e-02, -1.5216e-02,\n",
      "         -5.1599e-02,  1.4150e-01, -5.2842e-02,  1.0663e-01, -2.2569e-01,\n",
      "          1.2929e-01, -1.4755e-01, -3.7743e-02, -1.2537e-01, -2.0601e-02,\n",
      "         -1.6745e-01,  1.0024e-01,  2.0999e-03, -1.3182e-01, -8.9219e-02,\n",
      "          1.7615e-01,  5.2406e-02, -1.1129e-01,  3.5475e-02,  5.7380e-02,\n",
      "          4.0661e-02,  9.9997e-01,  1.3448e-01, -5.4220e-02,  8.7071e-02,\n",
      "          3.7370e-02, -9.9333e-02, -1.7698e-02, -2.0087e-01,  4.2966e-02,\n",
      "          1.5356e-02,  1.6801e-01,  4.0066e-02,  1.4903e-01,  5.3912e-02,\n",
      "         -1.6407e-02,  1.3879e-02, -3.2253e-02, -3.1300e-02, -3.1633e-02,\n",
      "         -9.9997e-01, -9.2345e-02,  9.3339e-02,  6.7181e-02, -6.3187e-02,\n",
      "          1.1357e-01, -1.8387e-01,  4.2646e-02, -1.5939e-01, -1.4703e-01,\n",
      "          3.8662e-02, -1.0885e-01, -2.6971e-02, -1.5611e-01, -1.1914e-01,\n",
      "         -7.4741e-02,  1.0031e-01, -3.8792e-02,  3.7001e-02,  6.0039e-02,\n",
      "          1.9909e-01,  1.1341e-01, -5.8861e-02,  1.2967e-01,  6.0274e-02,\n",
      "         -7.3983e-02, -1.4639e-01, -1.4569e-01, -8.1958e-02, -8.6242e-02,\n",
      "          8.0531e-02, -1.6287e-01,  1.1284e-01, -2.5736e-02,  9.4185e-03,\n",
      "          7.2501e-02,  6.8024e-02,  1.3849e-01, -8.0167e-02, -8.7095e-03,\n",
      "          8.9014e-02,  4.4210e-02, -1.0057e-01,  2.0387e-01, -2.2818e-01,\n",
      "          2.9170e-02,  1.1774e-01, -1.8447e-01,  3.2636e-02, -1.4952e-01,\n",
      "          1.7287e-02,  4.0376e-02,  3.6567e-02, -3.1295e-02, -1.7256e-01,\n",
      "         -6.1310e-02, -5.2161e-02,  1.6040e-01,  7.1988e-02, -7.3828e-02,\n",
      "         -8.7488e-02, -4.5352e-04, -8.6437e-02,  4.8172e-02, -1.1980e-01,\n",
      "         -9.9997e-01, -1.9672e-02, -1.3753e-01, -1.1484e-01,  8.7668e-02,\n",
      "         -5.5354e-02,  1.7402e-01, -1.7293e-01,  2.5024e-02,  1.5234e-01,\n",
      "          1.4683e-01, -1.0353e-01, -7.3260e-02, -6.2309e-02,  1.3338e-02,\n",
      "          1.8234e-02, -9.9997e-01, -6.2777e-02, -6.0438e-02, -6.4567e-02,\n",
      "          3.6171e-02,  1.0331e-01,  8.3357e-03, -7.1027e-02,  3.5080e-03,\n",
      "          9.2568e-02,  2.8476e-02, -1.5383e-02, -7.6432e-02, -1.3930e-01,\n",
      "         -7.4758e-02,  3.5826e-02, -3.6810e-02,  6.0947e-02,  2.8555e-02,\n",
      "         -7.7430e-02, -1.3311e-01,  2.0932e-01, -1.2894e-01,  2.0211e-01,\n",
      "          5.5587e-02,  3.7072e-02,  2.9124e-02, -4.0297e-02,  6.0058e-02,\n",
      "          7.4144e-02,  1.2833e-01, -1.9336e-01,  1.0839e-01, -9.7487e-02,\n",
      "         -4.9986e-02,  7.9610e-02,  7.6151e-02,  8.9523e-03,  1.4012e-01,\n",
      "         -5.4903e-02,  1.1086e-01,  6.4605e-02, -5.3065e-02, -7.8240e-02,\n",
      "          1.9312e-02, -1.3813e-01, -2.0409e-02, -1.7557e-02,  6.6604e-02,\n",
      "         -2.2191e-01, -5.5956e-02, -1.5415e-01,  1.4974e-02, -1.0508e-01,\n",
      "         -1.6425e-01, -7.1684e-02, -4.1270e-02,  4.1147e-02, -1.7863e-01,\n",
      "          7.8641e-02, -6.9877e-02, -1.1714e-01, -5.3147e-02,  2.7220e-02,\n",
      "          8.2003e-02,  9.6170e-02,  1.5444e-02,  3.1737e-02,  4.9428e-02,\n",
      "         -9.5874e-02,  7.8497e-02,  6.1233e-02, -2.3690e-03,  6.4237e-02,\n",
      "          7.5134e-02, -4.8381e-03, -1.2082e-01,  1.3322e-01,  7.7300e-02,\n",
      "         -3.9217e-02,  1.4754e-01, -7.3190e-02, -1.5133e-01,  9.5241e-02,\n",
      "         -3.6248e-02, -1.2820e-02,  1.2597e-01,  1.7707e-02, -5.1986e-02,\n",
      "          6.9664e-03,  9.0173e-02, -9.9996e-01,  2.3278e-01, -9.1241e-02,\n",
      "          5.7615e-02,  1.8967e-01,  2.3362e-01,  1.1054e-01, -6.3104e-03,\n",
      "          1.2567e-02, -1.9082e-01, -1.1230e-01,  1.2187e-01, -1.0882e-01,\n",
      "          1.0565e-01,  4.7997e-02, -1.6333e-01, -6.0820e-02,  3.1212e-02,\n",
      "          9.9853e-02, -1.2536e-01, -1.2859e-01, -1.4122e-02,  1.2615e-01,\n",
      "         -3.0178e-02, -6.1206e-02,  8.6262e-02,  3.0161e-02,  8.4119e-02,\n",
      "         -2.1167e-01, -1.8908e-01,  1.9277e-01,  1.2701e-01,  1.6737e-01,\n",
      "         -5.4464e-03,  1.6947e-01, -1.3425e-01, -7.8760e-02, -1.4082e-02,\n",
      "          3.8158e-02, -1.1174e-02,  2.1666e-02,  2.0641e-01, -2.1757e-03,\n",
      "         -1.3596e-01,  1.8011e-03, -1.8086e-01,  1.0332e-01, -8.5499e-02,\n",
      "          1.0046e-01,  5.7942e-03,  1.7124e-01, -1.1819e-01,  1.1932e-01,\n",
      "          6.4563e-02,  1.7956e-02, -6.1542e-02,  8.6949e-02, -3.9084e-02,\n",
      "          2.1545e-01, -8.1228e-03,  7.5140e-02,  1.1867e-01, -9.9997e-01,\n",
      "          9.9134e-03, -3.1940e-02, -6.9101e-02, -9.9997e-01,  4.2560e-02,\n",
      "          9.9636e-02,  2.0163e-02,  1.0619e-01,  1.3078e-01,  7.3780e-02,\n",
      "         -5.1286e-04, -5.3796e-02,  4.7441e-02,  2.2466e-02, -1.9815e-01,\n",
      "         -1.0020e-01,  6.1847e-02, -5.2199e-02, -1.3904e-01,  8.4409e-02,\n",
      "          6.9472e-02, -7.4022e-02,  2.1433e-01, -1.0741e-02,  3.0406e-01,\n",
      "         -3.2652e-02,  4.3389e-02, -1.2431e-01, -3.9371e-02, -2.1660e-02,\n",
      "         -1.3090e-01, -1.5555e-02,  3.5172e-03,  1.7926e-01, -8.5715e-02,\n",
      "         -1.2829e-01, -1.2779e-01, -6.6664e-02,  1.6291e-01,  3.9215e-02,\n",
      "          2.8569e-02, -5.3981e-03,  3.0826e-02, -8.0209e-02,  3.1097e-02,\n",
      "         -1.3675e-02,  7.2330e-02,  3.5667e-03, -3.7301e-02, -6.4758e-02,\n",
      "          1.6581e-01, -7.2102e-02,  8.5467e-02,  1.5604e-01,  1.1230e-01,\n",
      "         -9.4749e-02,  4.9170e-02, -5.3382e-02,  2.4912e-01, -9.9996e-01,\n",
      "          2.2446e-02, -8.6711e-03,  2.0833e-02, -6.7606e-02, -3.9333e-02,\n",
      "          2.6065e-01, -1.2756e-02, -1.8011e-02,  1.8289e-01, -9.5943e-02,\n",
      "         -4.4043e-02,  1.2162e-01,  1.1805e-01, -1.0138e-01,  2.6119e-02,\n",
      "         -2.3645e-02, -1.6438e-01, -1.0720e-01]], grad_fn=<TanhBackward0>), hidden_states=(tensor([[[ 0.0852, -0.1362, -0.5025,  ..., -0.0093,  0.0121, -0.0160],\n",
      "         [-0.7495,  0.4262, -0.3323,  ...,  0.1098,  0.6901,  0.2638],\n",
      "         [ 0.7105, -0.4261,  0.1650,  ..., -0.4462, -0.3033, -0.6886],\n",
      "         ...,\n",
      "         [ 0.4359,  0.2011,  0.4763,  ...,  0.6097, -1.6940,  1.3611],\n",
      "         [ 0.1792, -0.0962, -0.3525,  ...,  0.3006,  0.1500,  0.4358],\n",
      "         [ 0.4632, -0.1756,  1.0297,  ...,  0.1870, -0.2757,  0.0040]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0094,  0.0335,  0.1026,  ...,  0.0421, -0.0373,  0.0188],\n",
      "         [ 0.5988,  0.3806, -1.8592,  ...,  0.7229,  0.7661,  0.0906],\n",
      "         [ 1.4152, -0.2071, -1.0702,  ..., -0.0607, -0.6562, -0.6289],\n",
      "         ...,\n",
      "         [ 0.8508,  0.3830,  0.0023,  ...,  0.9051, -1.6294,  1.6058],\n",
      "         [ 0.1439,  0.4666,  0.5801,  ..., -0.0111, -0.0595,  0.6275],\n",
      "         [ 0.6441, -0.1495,  0.7339,  ...,  0.1540, -0.3807, -0.0133]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-2.5855e-02,  1.4637e-01,  1.6707e-01,  ..., -4.3336e-02,\n",
      "          -2.6447e-02, -1.4155e-02],\n",
      "         [ 5.8624e-01,  4.0281e-02, -1.8212e+00,  ...,  7.4987e-01,\n",
      "           3.7757e-01,  6.8543e-03],\n",
      "         [ 2.5469e+00,  4.4087e-01, -8.9541e-01,  ...,  2.0973e-01,\n",
      "          -8.4274e-01,  2.2605e-01],\n",
      "         ...,\n",
      "         [ 8.0662e-01,  2.1044e-01,  2.1208e-01,  ...,  3.9372e-01,\n",
      "          -1.1043e+00,  1.0303e+00],\n",
      "         [ 4.9275e-01,  5.0076e-01,  4.8073e-01,  ..., -3.3958e-01,\n",
      "          -1.2695e-01,  3.4977e-01],\n",
      "         [ 1.7542e-01,  1.4135e-02,  1.1057e+00,  ...,  3.3904e-04,\n",
      "          -1.7186e-01,  2.4249e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1052,  0.0824, -0.0831,  ..., -0.0862, -0.0385,  0.0252],\n",
      "         [-0.7417,  0.0188, -0.8727,  ...,  0.3896,  0.7296, -0.1569],\n",
      "         [-0.0652,  0.7464, -0.0781,  ...,  0.4491, -0.5693,  0.5373],\n",
      "         ...,\n",
      "         [ 0.3115,  0.1504,  0.4722,  ...,  0.3648, -0.3043,  1.0737],\n",
      "         [-0.1504,  0.8249,  0.6370,  ...,  0.0133,  0.0424,  0.5846],\n",
      "         [ 0.1812, -0.0268,  0.1564,  ...,  0.0099,  0.1093,  0.0345]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2270, -0.0146, -0.1563,  ..., -0.0390,  0.0275,  0.1980],\n",
      "         [-0.8495, -0.0488, -0.2996,  ...,  0.2717,  0.2058, -0.2341],\n",
      "         [-0.4219,  0.2677,  0.1113,  ...,  0.3359, -0.6507,  0.7366],\n",
      "         ...,\n",
      "         [ 0.2533,  0.1529,  0.2290,  ...,  0.3170, -0.4942,  1.0179],\n",
      "         [-0.0954,  0.6909,  0.5238,  ...,  0.0362,  0.0158,  0.4576],\n",
      "         [ 0.0496,  0.0376,  0.1196,  ...,  0.0365,  0.0715,  0.0487]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2594,  0.1669, -0.3437,  ..., -0.2763, -0.2022,  0.2322],\n",
      "         [-1.0021, -0.1861,  0.5041,  ...,  0.0091,  0.1387, -0.4650],\n",
      "         [-1.3696,  0.2716,  0.7368,  ..., -0.1117, -0.6408,  0.3262],\n",
      "         ...,\n",
      "         [ 0.2939,  0.4371,  0.5506,  ..., -0.0871, -0.4126,  0.7369],\n",
      "         [-0.3732,  0.9492,  0.5417,  ...,  0.0106,  0.1575,  0.6174],\n",
      "         [-0.0194, -0.0109,  0.0754,  ...,  0.0427,  0.0902,  0.0569]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.6080,  0.1348, -0.1071,  ..., -0.3363, -0.0031,  0.0857],\n",
      "         [-0.7981,  0.3971,  0.3206,  ...,  0.0786,  0.0708, -0.7939],\n",
      "         [-1.3229,  0.8545,  0.6220,  ..., -0.6019, -0.7582,  0.3144],\n",
      "         ...,\n",
      "         [ 0.1189,  0.8222,  0.4547,  ..., -0.1600, -0.4852,  0.7329],\n",
      "         [-0.0371,  1.0115,  0.2914,  ...,  0.1435,  0.0651,  0.7903],\n",
      "         [-0.0034,  0.0160,  0.0505,  ...,  0.0264,  0.0780,  0.0346]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3748,  0.1421, -0.4630,  ..., -0.3708, -0.0438,  0.0050],\n",
      "         [-1.2821, -0.0672,  0.5253,  ..., -0.3450,  0.1164, -0.6194],\n",
      "         [-1.1645,  0.2635,  0.4597,  ..., -0.6378, -0.4727,  0.3296],\n",
      "         ...,\n",
      "         [ 0.1391,  0.6226, -0.1195,  ..., -0.1657, -0.4071,  1.0545],\n",
      "         [-0.4617,  0.7363, -0.1726,  ...,  0.0636,  0.1338,  0.6378],\n",
      "         [ 0.0018, -0.0301,  0.0515,  ...,  0.0497,  0.0561,  0.0673]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.5004,  0.0976, -0.5884,  ..., -0.5542, -0.1261, -0.1726],\n",
      "         [-1.2227,  0.0028,  0.0498,  ..., -0.0100, -0.0937, -0.8077],\n",
      "         [-1.0487,  0.5620,  0.1299,  ..., -0.4425, -0.5428, -0.0446],\n",
      "         ...,\n",
      "         [ 0.3945,  0.7643,  0.2671,  ..., -0.3026,  0.1408,  0.8270],\n",
      "         [-0.0569,  0.8119, -0.4710,  ...,  0.1288,  0.0813,  0.1524],\n",
      "         [ 0.0291, -0.0325,  0.0170,  ...,  0.0149,  0.0229,  0.0444]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.4620, -0.0091, -0.6714,  ..., -0.4773, -0.3811, -0.1910],\n",
      "         [-0.7046,  0.2220, -0.2850,  ..., -0.2356,  0.1789, -0.8201],\n",
      "         [-0.4739,  0.7214, -0.2097,  ..., -0.5058, -0.0129,  0.0157],\n",
      "         ...,\n",
      "         [ 0.0442,  0.5960,  0.1499,  ..., -0.0681,  0.0027,  0.8414],\n",
      "         [-0.1080,  0.7732, -0.7951,  ...,  0.0542, -0.2293,  0.2306],\n",
      "         [ 0.0622, -0.0689, -0.0043,  ...,  0.0238,  0.0079,  0.0585]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2641, -0.3276, -0.9059,  ..., -0.4530, -0.4468, -0.1673],\n",
      "         [-0.9142,  0.0815, -0.4423,  ..., -0.4085, -0.1476, -0.4583],\n",
      "         [-0.5555,  0.5100, -0.5470,  ..., -0.8417, -0.1190,  0.1039],\n",
      "         ...,\n",
      "         [-0.0104,  0.5932, -0.0680,  ...,  0.0374, -0.2320,  0.7524],\n",
      "         [ 0.0444, -0.0090, -0.1190,  ...,  0.0121, -0.1274,  0.0964],\n",
      "         [-0.0512, -0.0644,  0.0851,  ...,  0.0326, -0.0102,  0.0496]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-4.5051e-01, -6.9180e-01, -8.4000e-01,  ..., -3.3732e-01,\n",
      "          -4.1468e-01, -1.7982e-01],\n",
      "         [-6.4420e-01,  2.7435e-01, -5.3940e-01,  ..., -1.9110e-01,\n",
      "          -2.8538e-01, -8.3154e-01],\n",
      "         [-5.1249e-01,  2.1609e-01, -4.5979e-01,  ..., -7.4780e-01,\n",
      "          -2.3803e-01, -1.5077e-02],\n",
      "         ...,\n",
      "         [ 1.6932e-01,  6.5801e-01, -2.3959e-01,  ..., -1.4525e-01,\n",
      "          -3.1066e-01,  5.8840e-01],\n",
      "         [-6.8399e-02, -3.4041e-02,  1.4221e-02,  ...,  1.0660e-02,\n",
      "          -2.4581e-04,  1.7568e-02],\n",
      "         [-8.5531e-02, -5.6249e-02,  4.3072e-02,  ...,  1.9647e-02,\n",
      "           3.5456e-02,  3.3575e-03]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0780, -0.0454, -0.0454,  ..., -0.0535, -0.0218, -0.0535],\n",
      "         [ 0.1749,  0.6618, -0.1256,  ..., -0.0904,  0.1683, -0.2197],\n",
      "         [-0.9231,  0.0877,  0.3768,  ..., -0.3216, -0.2217, -0.0118],\n",
      "         ...,\n",
      "         [-0.0898,  0.3302, -0.0684,  ..., -0.1578, -0.1485, -0.1357],\n",
      "         [-0.4545,  0.8917, -0.2299,  ...,  0.7790, -0.1886, -1.1163],\n",
      "         [-0.4443,  0.9073, -0.2187,  ...,  0.8316, -0.1154, -1.1627]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)), past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "text = read_examples(\"ObjCase_test.txt\")\n",
    "encoded_input = tokenizer(sentence, return_tensors='pt', padding = True)\n",
    "output = model(**encoded_input, output_hidden_states = True)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d70dba94-45cd-442e-8710-17dbfc325d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = output[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "144ed4b6-3a01-47ba-9d38-07491a975c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0852, -0.1362, -0.5025,  ..., -0.0093,  0.0121, -0.0160],\n",
      "         [-0.7495,  0.4262, -0.3323,  ...,  0.1098,  0.6901,  0.2638],\n",
      "         [ 0.7105, -0.4261,  0.1650,  ..., -0.4462, -0.3033, -0.6886],\n",
      "         ...,\n",
      "         [ 0.4359,  0.2011,  0.4763,  ...,  0.6097, -1.6940,  1.3611],\n",
      "         [ 0.1792, -0.0962, -0.3525,  ...,  0.3006,  0.1500,  0.4358],\n",
      "         [ 0.4632, -0.1756,  1.0297,  ...,  0.1870, -0.2757,  0.0040]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 71, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output = hidden_states[0]\n",
    "print(embedding_output)\n",
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57c8a067-4002-40ba-a74d-e44a3681beee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.0094,  0.0335,  0.1026,  ...,  0.0421, -0.0373,  0.0188],\n",
      "         [ 0.5988,  0.3806, -1.8592,  ...,  0.7229,  0.7661,  0.0906],\n",
      "         [ 1.4152, -0.2071, -1.0702,  ..., -0.0607, -0.6562, -0.6289],\n",
      "         ...,\n",
      "         [ 0.8508,  0.3830,  0.0023,  ...,  0.9051, -1.6294,  1.6058],\n",
      "         [ 0.1439,  0.4666,  0.5801,  ..., -0.0111, -0.0595,  0.6275],\n",
      "         [ 0.6441, -0.1495,  0.7339,  ...,  0.1540, -0.3807, -0.0133]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-2.5855e-02,  1.4637e-01,  1.6707e-01,  ..., -4.3336e-02,\n",
      "          -2.6447e-02, -1.4155e-02],\n",
      "         [ 5.8624e-01,  4.0281e-02, -1.8212e+00,  ...,  7.4987e-01,\n",
      "           3.7757e-01,  6.8543e-03],\n",
      "         [ 2.5469e+00,  4.4087e-01, -8.9541e-01,  ...,  2.0973e-01,\n",
      "          -8.4274e-01,  2.2605e-01],\n",
      "         ...,\n",
      "         [ 8.0662e-01,  2.1044e-01,  2.1208e-01,  ...,  3.9372e-01,\n",
      "          -1.1043e+00,  1.0303e+00],\n",
      "         [ 4.9275e-01,  5.0076e-01,  4.8073e-01,  ..., -3.3958e-01,\n",
      "          -1.2695e-01,  3.4977e-01],\n",
      "         [ 1.7542e-01,  1.4135e-02,  1.1057e+00,  ...,  3.3904e-04,\n",
      "          -1.7186e-01,  2.4249e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1052,  0.0824, -0.0831,  ..., -0.0862, -0.0385,  0.0252],\n",
      "         [-0.7417,  0.0188, -0.8727,  ...,  0.3896,  0.7296, -0.1569],\n",
      "         [-0.0652,  0.7464, -0.0781,  ...,  0.4491, -0.5693,  0.5373],\n",
      "         ...,\n",
      "         [ 0.3115,  0.1504,  0.4722,  ...,  0.3648, -0.3043,  1.0737],\n",
      "         [-0.1504,  0.8249,  0.6370,  ...,  0.0133,  0.0424,  0.5846],\n",
      "         [ 0.1812, -0.0268,  0.1564,  ...,  0.0099,  0.1093,  0.0345]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2270, -0.0146, -0.1563,  ..., -0.0390,  0.0275,  0.1980],\n",
      "         [-0.8495, -0.0488, -0.2996,  ...,  0.2717,  0.2058, -0.2341],\n",
      "         [-0.4219,  0.2677,  0.1113,  ...,  0.3359, -0.6507,  0.7366],\n",
      "         ...,\n",
      "         [ 0.2533,  0.1529,  0.2290,  ...,  0.3170, -0.4942,  1.0179],\n",
      "         [-0.0954,  0.6909,  0.5238,  ...,  0.0362,  0.0158,  0.4576],\n",
      "         [ 0.0496,  0.0376,  0.1196,  ...,  0.0365,  0.0715,  0.0487]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2594,  0.1669, -0.3437,  ..., -0.2763, -0.2022,  0.2322],\n",
      "         [-1.0021, -0.1861,  0.5041,  ...,  0.0091,  0.1387, -0.4650],\n",
      "         [-1.3696,  0.2716,  0.7368,  ..., -0.1117, -0.6408,  0.3262],\n",
      "         ...,\n",
      "         [ 0.2939,  0.4371,  0.5506,  ..., -0.0871, -0.4126,  0.7369],\n",
      "         [-0.3732,  0.9492,  0.5417,  ...,  0.0106,  0.1575,  0.6174],\n",
      "         [-0.0194, -0.0109,  0.0754,  ...,  0.0427,  0.0902,  0.0569]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.6080,  0.1348, -0.1071,  ..., -0.3363, -0.0031,  0.0857],\n",
      "         [-0.7981,  0.3971,  0.3206,  ...,  0.0786,  0.0708, -0.7939],\n",
      "         [-1.3229,  0.8545,  0.6220,  ..., -0.6019, -0.7582,  0.3144],\n",
      "         ...,\n",
      "         [ 0.1189,  0.8222,  0.4547,  ..., -0.1600, -0.4852,  0.7329],\n",
      "         [-0.0371,  1.0115,  0.2914,  ...,  0.1435,  0.0651,  0.7903],\n",
      "         [-0.0034,  0.0160,  0.0505,  ...,  0.0264,  0.0780,  0.0346]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3748,  0.1421, -0.4630,  ..., -0.3708, -0.0438,  0.0050],\n",
      "         [-1.2821, -0.0672,  0.5253,  ..., -0.3450,  0.1164, -0.6194],\n",
      "         [-1.1645,  0.2635,  0.4597,  ..., -0.6378, -0.4727,  0.3296],\n",
      "         ...,\n",
      "         [ 0.1391,  0.6226, -0.1195,  ..., -0.1657, -0.4071,  1.0545],\n",
      "         [-0.4617,  0.7363, -0.1726,  ...,  0.0636,  0.1338,  0.6378],\n",
      "         [ 0.0018, -0.0301,  0.0515,  ...,  0.0497,  0.0561,  0.0673]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.5004,  0.0976, -0.5884,  ..., -0.5542, -0.1261, -0.1726],\n",
      "         [-1.2227,  0.0028,  0.0498,  ..., -0.0100, -0.0937, -0.8077],\n",
      "         [-1.0487,  0.5620,  0.1299,  ..., -0.4425, -0.5428, -0.0446],\n",
      "         ...,\n",
      "         [ 0.3945,  0.7643,  0.2671,  ..., -0.3026,  0.1408,  0.8270],\n",
      "         [-0.0569,  0.8119, -0.4710,  ...,  0.1288,  0.0813,  0.1524],\n",
      "         [ 0.0291, -0.0325,  0.0170,  ...,  0.0149,  0.0229,  0.0444]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.4620, -0.0091, -0.6714,  ..., -0.4773, -0.3811, -0.1910],\n",
      "         [-0.7046,  0.2220, -0.2850,  ..., -0.2356,  0.1789, -0.8201],\n",
      "         [-0.4739,  0.7214, -0.2097,  ..., -0.5058, -0.0129,  0.0157],\n",
      "         ...,\n",
      "         [ 0.0442,  0.5960,  0.1499,  ..., -0.0681,  0.0027,  0.8414],\n",
      "         [-0.1080,  0.7732, -0.7951,  ...,  0.0542, -0.2293,  0.2306],\n",
      "         [ 0.0622, -0.0689, -0.0043,  ...,  0.0238,  0.0079,  0.0585]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2641, -0.3276, -0.9059,  ..., -0.4530, -0.4468, -0.1673],\n",
      "         [-0.9142,  0.0815, -0.4423,  ..., -0.4085, -0.1476, -0.4583],\n",
      "         [-0.5555,  0.5100, -0.5470,  ..., -0.8417, -0.1190,  0.1039],\n",
      "         ...,\n",
      "         [-0.0104,  0.5932, -0.0680,  ...,  0.0374, -0.2320,  0.7524],\n",
      "         [ 0.0444, -0.0090, -0.1190,  ...,  0.0121, -0.1274,  0.0964],\n",
      "         [-0.0512, -0.0644,  0.0851,  ...,  0.0326, -0.0102,  0.0496]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-4.5051e-01, -6.9180e-01, -8.4000e-01,  ..., -3.3732e-01,\n",
      "          -4.1468e-01, -1.7982e-01],\n",
      "         [-6.4420e-01,  2.7435e-01, -5.3940e-01,  ..., -1.9110e-01,\n",
      "          -2.8538e-01, -8.3154e-01],\n",
      "         [-5.1249e-01,  2.1609e-01, -4.5979e-01,  ..., -7.4780e-01,\n",
      "          -2.3803e-01, -1.5077e-02],\n",
      "         ...,\n",
      "         [ 1.6932e-01,  6.5801e-01, -2.3959e-01,  ..., -1.4525e-01,\n",
      "          -3.1066e-01,  5.8840e-01],\n",
      "         [-6.8399e-02, -3.4041e-02,  1.4221e-02,  ...,  1.0660e-02,\n",
      "          -2.4581e-04,  1.7568e-02],\n",
      "         [-8.5531e-02, -5.6249e-02,  4.3072e-02,  ...,  1.9647e-02,\n",
      "           3.5456e-02,  3.3575e-03]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0780, -0.0454, -0.0454,  ..., -0.0535, -0.0218, -0.0535],\n",
      "         [ 0.1749,  0.6618, -0.1256,  ..., -0.0904,  0.1683, -0.2197],\n",
      "         [-0.9231,  0.0877,  0.3768,  ..., -0.3216, -0.2217, -0.0118],\n",
      "         ...,\n",
      "         [-0.0898,  0.3302, -0.0684,  ..., -0.1578, -0.1485, -0.1357],\n",
      "         [-0.4545,  0.8917, -0.2299,  ...,  0.7790, -0.1886, -1.1163],\n",
      "         [-0.4443,  0.9073, -0.2187,  ...,  0.8316, -0.1154, -1.1627]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>))\n"
     ]
    }
   ],
   "source": [
    "attention_states = hidden_states[1:]\n",
    "print(attention_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "644be248-7249-4e74-a5f1-1144f14614df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0094,  0.0335,  0.1026,  ...,  0.0421, -0.0373,  0.0188],\n",
      "         [ 0.5988,  0.3806, -1.8592,  ...,  0.7229,  0.7661,  0.0906],\n",
      "         [ 1.4152, -0.2071, -1.0702,  ..., -0.0607, -0.6562, -0.6289],\n",
      "         ...,\n",
      "         [ 0.8508,  0.3830,  0.0023,  ...,  0.9051, -1.6294,  1.6058],\n",
      "         [ 0.1439,  0.4666,  0.5801,  ..., -0.0111, -0.0595,  0.6275],\n",
      "         [ 0.6441, -0.1495,  0.7339,  ...,  0.1540, -0.3807, -0.0133]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attention_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3545e95-b6d9-4c28-961f-4acf7739ba19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 71, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbc510cd-c5b9-4aba-8221-015ee48c8dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([71, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_states[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43cef7b3-6f7f-4a85-95ca-bca38f8f4f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.4085e-03,  3.3546e-02,  1.0265e-01,  1.2798e-01, -1.9224e-02,\n",
       "         1.0460e-01,  1.1069e-01,  3.6181e-01, -7.9835e-02, -4.3085e-03,\n",
       "         6.0830e-02,  3.3146e-02, -2.1032e-01, -3.7836e-02, -2.2753e-03,\n",
       "         1.1189e-01,  1.1779e-01,  2.8227e-02, -2.8524e-02, -1.7341e-01,\n",
       "        -1.7848e-01, -3.8752e-02,  1.4356e-01, -2.1748e-02,  1.4220e-01,\n",
       "         1.7719e-01,  1.3729e-02,  1.5290e-02,  1.0853e-01, -3.6514e-02,\n",
       "         3.0824e-01, -1.3306e-01,  1.1206e-01, -2.4077e-01, -2.5155e-02,\n",
       "        -3.5963e-02,  1.0429e-01,  8.2572e-02, -1.0377e-01, -8.2127e-03,\n",
       "         3.5695e-02,  2.0518e-02,  1.5452e-01,  1.1102e-01, -1.5344e-02,\n",
       "         2.3688e-02,  1.4643e-01,  4.7333e-02, -2.5476e-03, -1.1751e-01,\n",
       "        -1.6738e-01, -6.3824e-02,  7.1502e-02, -1.3477e-02, -6.3210e-02,\n",
       "        -2.0636e-02,  8.5308e-02,  4.2832e-02,  3.0593e-02, -1.7998e-02,\n",
       "         1.5114e-01,  7.5114e-02,  1.1214e-01, -5.4157e-02, -4.4790e-02,\n",
       "         3.2222e-02,  5.7397e-02, -1.9898e-01,  1.1235e-02, -1.7806e-02,\n",
       "        -1.2191e-01, -8.9270e-02, -5.5055e-02, -8.8498e-03, -8.2810e-02,\n",
       "        -1.4542e-01, -1.1220e-01, -5.6934e-02,  6.1484e-02, -1.4691e-01,\n",
       "         6.0241e-02, -5.2844e-02,  7.7474e-02,  3.7372e-01,  3.1251e-02,\n",
       "        -1.6627e-02, -6.4100e-02,  7.6709e-03, -4.1144e-02, -6.9542e-03,\n",
       "         4.0763e-02, -2.0460e-01, -2.1134e-01,  1.5794e-02, -1.0647e-01,\n",
       "        -4.3621e-02,  1.6873e-02,  3.5849e+00,  1.4353e-01, -4.3199e-02,\n",
       "        -1.5981e-02,  8.7010e-02,  5.6076e-02, -1.2693e-01, -6.6106e-02,\n",
       "         1.8133e-01,  9.0675e-02,  3.3213e-02, -1.6103e-01, -2.7377e-01,\n",
       "        -6.7355e-03, -1.7100e-02,  1.2555e-01, -5.9983e-02,  8.7444e-03,\n",
       "        -2.4528e-01, -5.9742e-02, -5.2895e-02,  1.0478e-01,  8.4126e-02,\n",
       "        -6.7868e-02, -1.9325e-02, -1.2386e-01, -1.8198e-02,  2.1522e-01,\n",
       "         5.5573e-02,  2.1446e-01,  4.2513e-03, -1.0387e-01,  1.9130e-01,\n",
       "         1.4934e-02,  1.4058e-02,  9.7683e-02,  3.7301e-05,  6.3912e-02,\n",
       "         6.9597e-04, -2.2475e-02, -1.7119e-02,  6.2668e-02, -9.0707e-02,\n",
       "         7.4060e-02, -3.1341e-01,  6.3474e-02,  7.4735e-02,  7.8344e-02,\n",
       "        -1.3108e-02, -8.2800e-02, -8.2688e-03, -2.0870e-01, -3.0685e-02,\n",
       "         1.1897e-01, -1.9673e-03,  8.6416e-02, -2.4421e-02, -5.5333e-02,\n",
       "         2.5546e-01,  9.3435e-03,  6.8055e-02,  6.7470e-02, -1.2012e+01,\n",
       "         1.1115e-01,  5.5971e-02, -2.3684e-02, -8.4109e-02, -4.5568e-02,\n",
       "         7.1630e-05, -2.6395e-01,  1.5895e-01,  5.1289e-03,  1.3748e-01,\n",
       "        -1.6280e-01,  8.3640e-02,  1.4740e-02, -9.0495e-02, -9.1437e-02,\n",
       "         1.3601e-02,  3.5710e-02, -7.8975e-02, -3.7511e-02, -5.1723e-02,\n",
       "        -8.6077e-02,  1.5164e-01, -2.1904e-02,  2.6803e-02, -1.2520e-02,\n",
       "        -7.8779e-02,  8.5985e-02, -3.5606e-02, -2.1923e-02,  2.6768e-02,\n",
       "         1.1273e-02,  6.4527e-03, -5.4554e-02,  6.2586e-02, -1.0050e-01,\n",
       "        -5.4101e-02,  2.4542e-02,  1.7698e-01, -1.0029e-01,  7.0799e-02,\n",
       "        -4.4702e-02, -3.0294e-02,  1.7885e-02, -2.0950e-02,  7.3433e-02,\n",
       "        -9.7808e-02, -1.3489e-01, -6.6323e-03,  8.1506e-02, -1.0464e-01,\n",
       "        -1.2451e-01, -9.2561e-02, -1.4131e-01,  1.1256e-01, -4.5885e-02,\n",
       "        -1.8490e-01, -4.6454e-02,  9.3701e-02, -1.3152e-02, -1.7304e-02,\n",
       "        -1.7421e-01,  6.6177e-02,  9.8569e-02,  2.4572e-02, -1.3508e-01,\n",
       "        -2.1620e-01, -2.6918e-02,  1.0187e-01, -8.8128e-03,  1.2287e-02,\n",
       "         5.1481e-02,  6.6174e-02, -1.0784e-01,  1.4651e-02,  1.2635e-01,\n",
       "        -4.1774e-02,  9.2638e-03, -1.6183e-01, -8.4063e-02, -2.2260e-02,\n",
       "         5.9564e-02, -6.5438e-02,  2.4510e-02,  4.7761e-02,  3.0220e-01,\n",
       "        -7.3585e-02, -5.0459e-02, -2.7837e-02,  2.5172e-02, -3.4958e-02,\n",
       "        -8.1932e-03, -1.2881e-01, -6.0498e-01,  1.7995e-02,  2.2967e-01,\n",
       "        -6.9231e-02, -1.5269e-02,  3.4479e-02,  8.5224e-03,  1.5096e-01,\n",
       "        -7.8043e-02,  5.6964e-02, -7.7788e-02,  1.3755e-01, -1.5685e-01,\n",
       "        -2.5655e-02,  4.6690e-02, -1.8435e-02, -6.8998e-03,  4.4476e-02,\n",
       "        -8.4892e-02,  3.1227e-03, -1.9089e-01,  1.1335e-02, -7.1372e-02,\n",
       "        -4.2539e-02,  9.3298e-02,  4.3914e-02,  1.0153e-01,  1.3582e-01,\n",
       "         1.5624e-01, -5.9585e-02,  2.2205e-02, -1.6504e-01, -3.3788e-02,\n",
       "        -8.2343e-02, -6.4454e-02,  8.9710e-02, -4.8236e-02,  6.6391e-02,\n",
       "        -4.1528e-01,  4.3113e-02, -7.8050e-03, -7.2041e-03,  5.9154e-01,\n",
       "         4.1872e-02, -1.0857e-01, -5.9950e-02, -4.2733e-02,  7.5910e-02,\n",
       "        -9.2535e-02, -7.4965e-02, -6.8003e-03,  9.2061e-04, -2.6905e-02,\n",
       "         8.8538e-02, -4.7006e-02, -2.6191e-02, -7.6567e-02,  2.5623e-02,\n",
       "        -4.8803e-03, -6.3813e-02,  4.0636e-03, -4.1289e-03,  4.6420e-02,\n",
       "         1.4922e-01, -3.1961e-02,  7.7098e-03, -7.6672e-02,  1.0281e-01,\n",
       "         7.4030e-02,  1.0628e-01,  9.5725e-03,  1.8664e-01,  1.5490e-01,\n",
       "        -1.0463e-01,  1.5160e-03,  1.5063e-01,  1.0256e-01,  4.0920e-01,\n",
       "         2.3206e-02, -2.0262e-02,  1.7404e-01, -7.9861e-02, -1.3635e-01,\n",
       "        -1.5081e-01,  6.6514e-02,  2.2818e-02, -1.7592e-01, -9.5846e-02,\n",
       "        -1.7388e-02,  7.3216e-02,  7.2825e-02, -2.2393e-02,  1.7736e-02,\n",
       "         8.2554e-02,  4.2991e-02,  1.9227e-01,  1.9314e-01, -5.3143e-03,\n",
       "         2.4371e-02,  9.8382e-03,  5.1472e-02, -5.3425e-03, -1.0156e-01,\n",
       "         2.0876e-01, -9.5376e-02, -9.7815e-02, -9.5376e-02, -3.5981e-02,\n",
       "         2.0650e-02, -4.0966e-02,  1.6327e-01,  5.5114e-02,  3.3000e-02,\n",
       "         2.1752e-01, -3.0679e-01,  4.9810e-02, -1.2986e-01,  1.4942e-02,\n",
       "        -6.0340e-02,  1.5197e-01,  1.8887e-01, -1.5034e-01,  6.1246e-02,\n",
       "        -5.0128e-02, -7.9195e-02, -4.5345e-02, -5.1024e-02,  1.3236e-01,\n",
       "        -4.5624e-02,  2.3669e-01,  1.2889e-01, -3.9136e-02, -1.6196e-01,\n",
       "         3.5926e-02, -1.1674e-02,  1.5141e-01, -9.3381e-02, -4.0316e-02,\n",
       "        -4.8292e-03,  5.6872e-02, -3.0395e-02, -8.2449e-02,  3.8582e-02,\n",
       "         4.5968e-02,  1.3227e-01,  3.8544e-02,  4.5651e-02,  1.3680e-01,\n",
       "        -1.8044e-01, -8.1714e-02,  8.9739e-04,  1.3621e-01,  1.0523e-01,\n",
       "        -1.1606e-02, -8.5587e-02, -2.4232e-02,  1.2603e-01, -5.6723e-02,\n",
       "        -7.2615e-02,  9.3171e-02, -2.4482e-02,  9.7182e-02,  1.0086e-01,\n",
       "         5.8813e-02,  5.5316e-02,  3.6906e-03, -1.0759e-01,  1.1214e-01,\n",
       "        -1.3116e-01, -4.7676e-01, -4.5198e-02, -2.0200e-02,  2.6363e-02,\n",
       "        -6.4964e-03, -6.7734e-03, -4.7260e-02,  2.5133e-01, -8.0336e-02,\n",
       "         3.3659e-02,  1.3093e-01,  1.5856e-01,  4.9056e-02, -3.8697e-03,\n",
       "         3.1345e-02, -7.3351e-03, -3.7517e-02, -1.2178e-02, -1.7040e-02,\n",
       "         6.6696e-02, -2.7786e-01, -3.3196e-02,  7.3649e-02, -9.6601e-02,\n",
       "        -4.2551e-02,  4.0058e-03,  1.9811e-01, -6.0086e-02, -6.8899e-03,\n",
       "        -1.9668e-02, -3.1583e-02,  3.8106e-02, -2.9751e-01, -6.4222e-02,\n",
       "        -4.2749e-02,  2.4058e-01,  3.0825e-02, -1.9334e-01,  2.0478e-01,\n",
       "        -1.0292e-02,  1.1845e-01, -5.2723e-02,  1.5475e-01, -5.2480e-02,\n",
       "        -1.1052e-02, -1.0298e-02,  2.8320e-01,  1.5119e-02,  9.3690e-02,\n",
       "        -9.4689e-02,  7.6908e-02,  4.1958e-02,  5.5524e-02,  1.7645e-02,\n",
       "        -1.6071e-02,  1.0900e-01, -2.2076e-02,  8.7909e-02,  1.1136e-01,\n",
       "         2.5025e-01, -4.7715e-02, -1.7733e-01, -6.8882e-02, -5.3064e-02,\n",
       "         7.0978e-02, -1.2076e-01,  1.5425e-01, -4.2412e-02, -7.4056e-02,\n",
       "         1.6239e-01, -3.4924e-02,  2.4777e-01,  1.7949e-01, -9.9462e-02,\n",
       "         1.2712e-01, -1.2855e-01, -1.3292e-01, -1.1968e-01,  1.8954e-01,\n",
       "         2.2810e-02, -8.7227e-03,  1.8475e-01,  1.5189e-01, -6.7016e-02,\n",
       "         1.2258e-01, -3.8207e-02, -1.0680e-02,  4.8076e-02, -1.0952e-02,\n",
       "         4.8665e-02,  1.0425e-01,  1.4428e-01,  9.4317e-02, -7.9754e-02,\n",
       "         3.6368e-02, -1.8692e-01, -5.8150e-02, -4.3400e-03, -6.9627e-02,\n",
       "        -1.0732e-03,  1.6909e-01,  1.3664e-02,  2.0840e-01, -4.9146e-02,\n",
       "         1.4090e-01, -1.3832e-01,  8.6737e-03, -1.7317e-01,  1.0572e-01,\n",
       "         1.1469e-01, -1.0407e-02, -7.6731e-02,  1.6559e-03, -1.0899e-01,\n",
       "         1.1747e-01, -4.1276e-02,  9.1682e-02, -6.3464e-02,  2.8158e-01,\n",
       "        -3.4257e-02,  3.0881e-02,  6.3641e-02,  1.0788e-01,  1.2663e-01,\n",
       "        -8.3999e-02, -5.1084e-02, -1.2846e-02, -6.9685e-02, -1.4533e-01,\n",
       "        -5.9064e-02,  4.7562e-02, -5.3940e-02,  2.9908e-02, -1.9771e-01,\n",
       "         1.2920e-01, -8.4416e-02,  3.4030e-01, -7.0495e-02, -9.4060e-02,\n",
       "         1.7781e-01, -1.4182e-01,  8.7254e-02, -2.6371e-01, -1.5516e-01,\n",
       "        -6.4311e-02, -3.9821e-02, -1.0378e-01,  8.6281e-02,  4.7484e-02,\n",
       "         5.5563e-02,  6.1920e-02,  1.5398e-02, -6.2696e-02, -2.4352e-02,\n",
       "        -5.0469e-02,  6.5700e-03,  6.5076e-02, -1.2828e-01,  2.7995e-01,\n",
       "        -9.2717e-02, -9.0538e-02, -1.2068e-01,  5.2810e-02, -2.6272e-02,\n",
       "        -1.1460e-02,  1.3647e-01, -1.1498e-02,  2.0582e-02, -5.5711e-02,\n",
       "        -7.2217e-02, -2.2456e-01, -4.2055e-03,  4.2780e-03,  2.2511e-01,\n",
       "         1.7604e-01, -5.9459e-02,  4.8213e-02,  7.9109e-02,  1.5713e-01,\n",
       "        -1.2418e-01,  7.1971e-02, -9.8548e-02, -2.4972e-02,  6.4424e-02,\n",
       "        -1.5090e-01, -1.8635e-01,  3.5480e-02, -1.7068e-02,  1.5336e-01,\n",
       "         9.5789e-02,  7.1280e-02, -4.0866e-02, -4.0739e-02, -7.9072e-02,\n",
       "         3.1933e-02, -1.6924e-01, -4.4356e-02,  7.4335e-02, -1.1414e-01,\n",
       "        -6.6036e-04,  4.4114e-02, -6.3814e-03,  2.3082e-01, -2.7798e-01,\n",
       "        -1.3946e-01, -9.8082e-02, -1.6727e-01, -1.0164e-03,  1.9432e-01,\n",
       "        -1.0958e-01,  4.7539e-02, -5.2457e-02,  4.4867e-03, -1.2221e-02,\n",
       "         7.0422e-02,  2.9013e-01, -9.9929e-02,  2.1198e-01,  1.2854e-01,\n",
       "         5.2760e-02,  4.8414e-02, -7.4283e-02,  9.1699e-02,  3.7663e-02,\n",
       "        -2.5313e-02,  6.3958e-02, -8.0568e-02,  2.6362e-02,  4.6532e-02,\n",
       "        -3.1831e-01, -9.3844e-02,  3.7513e-02, -3.4258e-02, -1.8934e-01,\n",
       "         7.1276e-02,  7.7789e-03, -9.3669e-03, -1.2868e-01, -2.4235e-02,\n",
       "        -3.0529e-02, -1.6557e-02,  2.0519e-02, -9.6324e-02,  4.1169e-02,\n",
       "        -1.1751e-01,  3.0073e-01,  1.6974e-02,  2.0901e-01, -1.1896e-01,\n",
       "         4.1433e-04,  9.7978e-02,  1.7699e-02,  9.5048e-02, -1.3971e-02,\n",
       "         5.1140e-03,  2.6398e-01, -2.3442e-03,  1.2948e-01,  7.3320e-02,\n",
       "         5.0626e-02,  1.0797e-01, -1.0457e-01,  7.1205e-02, -1.3671e-02,\n",
       "         1.3626e-02,  5.5892e-02, -3.6267e-02, -1.3777e-01, -2.9320e-02,\n",
       "        -2.4572e-02, -4.3526e-02,  1.2287e-02, -4.4871e-02, -6.5610e-03,\n",
       "         4.5302e-02,  2.4663e-02, -2.5044e-02,  8.6488e-01, -4.1591e-02,\n",
       "         1.1213e-01, -2.9779e-02, -1.5941e-01, -1.8671e-01, -3.7249e-02,\n",
       "        -4.7212e-02, -2.1912e-02, -4.2304e-02,  1.7937e-01,  1.0364e-01,\n",
       "         5.9534e-02, -8.8097e-02, -1.1773e-01,  6.8529e-02, -3.0702e-02,\n",
       "         1.3334e-02, -3.9345e-02, -1.2006e-01,  1.2983e-01,  1.7381e-01,\n",
       "        -9.0775e-02,  1.4370e-01, -2.1110e-01, -1.6112e-01,  4.7854e-02,\n",
       "        -7.3608e-02,  6.3736e-02,  7.5819e-02, -7.5181e-02,  1.9186e-02,\n",
       "        -1.7696e-01, -8.1994e-03,  4.6278e-02, -4.9112e-02, -5.2977e-02,\n",
       "        -6.6367e-02, -2.2956e-01,  4.5178e-02,  3.0073e-02, -1.1873e-01,\n",
       "         7.9412e-02,  4.7978e-01, -2.3764e-02, -8.7104e-02,  1.8445e-01,\n",
       "        -9.1056e-02, -4.2812e-02, -3.1698e-02, -2.8713e-02, -4.0800e-02,\n",
       "         5.2439e-02,  1.7879e-01,  1.2733e-01,  2.6787e-02, -1.4901e+00,\n",
       "        -1.1624e-02, -1.1206e-02,  1.3996e-01,  5.3108e-02,  1.3807e-01,\n",
       "         1.0441e-01,  1.2586e-01, -2.0557e-04, -3.8535e-02,  1.2635e-01,\n",
       "         4.2100e-02, -3.7286e-02,  1.8814e-02], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_states[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abe8c175-debc-4cd0-8ccd-507b592b2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0780, -0.0454, -0.0454,  ..., -0.0535, -0.0218, -0.0535],\n",
      "         [ 0.1749,  0.6618, -0.1256,  ..., -0.0904,  0.1683, -0.2197],\n",
      "         [-0.9231,  0.0877,  0.3768,  ..., -0.3216, -0.2217, -0.0118],\n",
      "         ...,\n",
      "         [-0.0898,  0.3302, -0.0684,  ..., -0.1578, -0.1485, -0.1357],\n",
      "         [-0.4545,  0.8917, -0.2299,  ...,  0.7790, -0.1886, -1.1163],\n",
      "         [-0.4443,  0.9073, -0.2187,  ...,  0.8316, -0.1154, -1.1627]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "text = read_examples(\"ObjCase_test.txt\")\n",
    "encoded_input = tokenizer(text, return_tensors='pt', padding = True)\n",
    "output = model(**encoded_input, output_hidden_states = True)\n",
    "\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc9b25a-8d7e-43be-b580-82996b3df79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 71, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bcde652-020a-451c-aa57-783d2148d77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 71, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a99609e-2763-4d6d-9cc5-5ff6a3853379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "text = read_examples(\"ObjCase.txt\")\n",
    "with open(\"ObjCase_multi.json\", \"w\", encoding='utf-8') as writer:\n",
    "    for sentence in text:\n",
    "        encoded_input = tokenizer(sentence, return_tensors='pt', padding = True)\n",
    "        output = model(**encoded_input, output_hidden_states = True)\n",
    "        hidden_states = output[2][1:]\n",
    "        output_json = collections.OrderedDict()\n",
    "        output_json[\"linex_index\"] = text.index(sentence)\n",
    "        all_out_features = []\n",
    "        all_layers = []\n",
    "        for layer_index in range(len(hidden_states)):\n",
    "            layer_output = hidden_states[int(layer_index)].detach().cpu().numpy()\n",
    "            layer_output = layer_output[0, 0, :]\n",
    "            layers = collections.OrderedDict()\n",
    "            layers[\"index\"] = layer_index\n",
    "            layers[\"values\"] = [\n",
    "                round(x.item(), 6) for x in layer_output\n",
    "            ]\n",
    "            all_layers.append(layers)\n",
    "        out_features = collections.OrderedDict()\n",
    "        out_features[\"token\"] = \"[CLS]\"\n",
    "        out_features[\"layers\"] = all_layers\n",
    "        all_out_features.append(out_features)\n",
    "        output_json[\"features\"] = all_out_features\n",
    "        writer.write(json.dumps(output_json) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a05632-4f2a-4517-a0b6-2c361b700ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
